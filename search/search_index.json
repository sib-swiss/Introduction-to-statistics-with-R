{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction to Statistics and Data Visualisation with R","text":""},{"location":"#teachers","title":"Teachers","text":"<p>Rachel Marcone</p> <p>Joao Lourenco</p>"},{"location":"#helper","title":"Helper","text":"<p>Tania Wyss</p>"},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>At the end of this course, participants will be able to: - have experience in the application of basic statistics techniques - know how to summarize data with numerical and graphical summaries - plot data - do hypothesis testing and multiple testing correction - linear models - correlation and regression - principal component analysis and other topics</p>"},{"location":"#learning-outcomes-explained","title":"Learning outcomes explained","text":"<p>To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>The course will combine lectures on statistics and practical exercises, during which the participants will learn how to work with the widely used \u201cR\u201d language and environment for statistical computing and graphics.</p> <p>Participants will also have the opportunity to ask questions about the analysis of their own data.</p>"},{"location":"#exercises","title":"Exercises","text":"<p>Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. Some answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.</p>"},{"location":"#asking-questions","title":"Asking questions","text":"<p>During lectures, you are encouraged to raise your hand if you have questions.</p>"},{"location":"bonus_code/","title":"Bonus code","text":""},{"location":"bonus_code/#bonus-code","title":"Bonus code","text":""},{"location":"course_schedule/","title":"Course schedule","text":"<p>Note</p> <p>Apart from the starting time the time schedule is indicative. Because we can not plan a course by the minute, in practice the time points will deviate. </p>"},{"location":"course_schedule/#day-1","title":"Day 1","text":"block start end subject introduction 9:00 AM 9:30 AM Introduction to us and the group block 1 9:30 AM 10:30 AM Introduction to summary statistics 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Introduction to hypothesis testing 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Exercises"},{"location":"course_schedule/#day-2","title":"Day 2","text":"block start end subject block 1 9:00 AM 10:30 AM Parametric and non parametric tests 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM ANOVA 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Exercises"},{"location":"course_schedule/#day-3","title":"Day 3","text":"block start end subject block 1 9:00 AM 10:30 AM Correlation and simple regression 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Multiple regressions 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Exercises"},{"location":"course_schedule/#day-4","title":"Day 4","text":"block start end subject block 1 9:00 AM 10:30 AM Clustering 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM PCA 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Exercises"},{"location":"day1/","title":"Descriptive Statistics and Exploratory Data Analysis","text":""},{"location":"day1/#learning-outcomes-of-the-day","title":"Learning outcomes of the day","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Start an analysis in R / First steps in R</li> <li>Visualise your data with a few simple commands</li> <li>Summarise your data</li> <li>Load data into R from a file (for example a CSV file)</li> <li>Understand basic plotting</li> </ul>"},{"location":"day1/#material","title":"Material","text":"<p>In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises.</p> <p>Slides of lectures: Download slides Introduction Lecture</p> <p>Download slides Morning Lecture</p> <p>Download slides Help for first steps in R</p> <p>Download slides Afternoon Lecture</p> <p>Data for exercises:</p> <p>Download full data for the week</p>"},{"location":"day1/#first-hands-on-in-r","title":"First hands on in R","text":"<p>The purpose of this exercise is to introduce you to using R for statistical analysis of data. Code that you can copy and paste is provided here to get you started. You will get the most out of the session if you also do some exploring on your own: check the help files for each function to learn what default values and optional arguments are there, and try out your own variations.</p>"},{"location":"day1/#preliminaries-getting-help-in-r","title":"Preliminaries: Getting help in R","text":"<p>To proceed, you will need to start R. We asked you to install RStudio as this is an integrated development environment that makes it easy to write, debug, and visualize data analysis and statistical computing projects in R.</p> <p>You should be able to see the following page</p> <p>In red, you can find the console. Inside, we will write all the commands. They need to come after the \u201c&gt;\u201d sign.</p> <p>For best practice always open a new Rscript to write all your commands that you run in your console. This enables reproducibility and follows the FAIR rules (Findable, Accessible, Interoperable, and Reusable). For that press \u201cFile/New File/RScript\u201d. </p> <p>You now have on the top left, the Untitled (try to save it!) file that you can later re-open when you want to come back to this script.</p> <p>On the top right, you can find the environment, i.e. all the available objects. At the moment, this is empty. </p> <p>Try to write the first command, just a calculation</p> <pre><code>2+2\n</code></pre> <p>The # sign indicates a comment: anything occurring after this sign on a line is ignored by R (but can be very useful in programming at it provides a means for documenting your code). </p> Hint <p>Practice this throughout the course!</p> <p>Therefore the following line gives exactly the same result as before. </p> <pre><code>2+2 # This is just a simple calculation \n</code></pre> <p>You should become acquainted with the help facility within R, it can be your friend! The basic help command is</p> <pre><code>help()\n</code></pre> <p>within the parentheses you would type (inside of double quotes) the name of a function whose help file you want to see, e.g.</p> <pre><code>help(\"mean\")\n</code></pre> <p>You can also use the alternative syntax  <pre><code>?mean\n</code></pre></p> <p>If you don\u2019t know the exact command name, use</p> <pre><code>help.search()\n</code></pre> <p>with the name of the concept inside double quotes within the parentheses.</p>"},{"location":"day1/#getting-data-into-r","title":"Getting Data into R","text":"<p>R has a number of functions to create data vectors, including: c(), seq(), rep(). Find out what each of these do, and make some data vectors of your choice using each.</p> <p>To get some practice using statistical functions and performing small calculations in R, create a weight and corresponding height vector for computing body mass index (bmi) (this example is inspired by Dalgaard\u2019s book, Introductory statistics with R):</p> <pre><code>weight &lt;- c(65,72,55,91,95,72)\nheight &lt;- c(1.73, 1.80, 1.62, 1.90, 1.78, 1.93)\nbmi &lt;- weight / height^2\nbmi # Type this in R to see the computed values\n</code></pre> <p>These data vectors are a little too small to really require summaries. It is a little more interesting to look at real data.</p>"},{"location":"day1/#hellung-data","title":"Hellung Data","text":"<p>We are going to load the package ISwR, and examine the variables in the data set hellung.</p> <p>First, we need to make sure the package is installed. From R Studio, you can go to the menu Tools -&gt; Install packages\u2026, and then choose the package you need installed. Using the RGui under Windows, you can go to menu Packages -&gt; Install package(s) In the console, you can use the install.packages command: install.packages(\u201cISwR\u201d). R packages have an explanation on installation, which you can find in each help manual of the package. Once installed you can load the library as well as the data hellung.</p> <pre><code>library(ISwR)\n?hellung\ndata(hellung)\n</code></pre>"},{"location":"day1/#univariate-numerical-summaries","title":"Univariate numerical summaries","text":"<p>You can find the variable names with  <pre><code>names(hellung)\n</code></pre> \u2026 and can summarize the data set with </p> <pre><code>summary(hellung)\n</code></pre> <p>All good ?</p> <p>Also compute the mean and sd for each variable. Which of the variables does it not make sense to summarize like this?</p>"},{"location":"day1/#univariate-graphical-summaries","title":"Univariate graphical summaries","text":"<p>Make histograms of each of the variables.</p> <pre><code>par(mfrow=c(2,2)) # for viewing multiple plots (2 rows x 2 columns = 4 plots)\nhist(hellung$conc)\nhist(hellung$diameter)\nhist(hellung$glucose)\n</code></pre> <p>Make a boxplot of the variable conc. Now, make side by side boxplots of conc, one for each value of glucose; do the same with diameter. Note: the conc ~ glucose notation means \u201cexplain conc according to glucose\u201d; it tells R that it should split the boxplot according to the different values of the \u201cglucose\u201d variable.</p> <pre><code>par(mfrow=c(2,2)) # for viewing multiple plots\nboxplot(conc ~ glucose, data=hellung)\nboxplot(diameter ~ glucose, data=hellung)\n</code></pre> <p>Does the distribution (pattern of variability) of either variable appear to depend on the presence or absence of glucose? Do we have enough information to decide whether glucose is causing any difference?</p>"},{"location":"day1/#a-bivariate-look","title":"A bivariate look","text":"<p>It is also interesting to further explore relationships between different variables. We have already looked informally at the relationship between glucose and the other variables. We can also explore the relationship between the numerical variables conc and diameter:</p> <pre><code>cor(hellung$conc, hellung$diameter)\nplot(diameter ~ conc, data=hellung)\n</code></pre> <p>Do you see any structure in the scatterplot? What happens if we take log(conc) instead of conc?</p>"},{"location":"day1/#importing-and-exporting-data-into-r","title":"Importing and exporting data into R","text":"<p>Usually, the data to be analysed in R is already available in another program, typically Excel, and must be imported into R.</p> <p>You can read many different file formats in R, including text files and Excel files. However, since Excel files can be complex (including, for example, merged cells that are hard to understand), it is recommended in most cases to export them to text format first, either \u201cCSV\u201d (Comma-separated variables) or \u201ctab delimited\u201d, and to make sure that the result is correct, before loading them into R.</p> <p>Typical R commands for reading these files are read.table, read.delim, read.csv. The help pages can tell you the differences between these commands, but read.csv is the one to use for CSV files.</p> <p>One important caveat is the configuration of your computer with regards to the decimal point: if Excel saves files using commas for the decimal separator (e.g. 10,00 instead of 10.00), R will not recognize the data as numbers because of the \u201cparasite\u201d character. The option dec = \u201c,\u201d can be used if necessary to modify this behaviour.</p> <p>Conversely, the write.table command can be used to write a table to a file for subsequent reading into Excel. When using R studio, you can use the \u201cimport dataset\u201d tool, that will allow you to explore the structure of the data you import. A useful feature of this tool is that, when finished, it will not only load the data, but will also print the actual R command that was used to do so, allowing you to copy it to your script for future use. Note: recent versions of R Studio load data into a variable that is not a data frame, but a more advanced structure. The resulting variable works mostly like a data frame, but there are some differences. If you have any issue, try converting it back to a data frame. For example, if you loaded data using the importer tool, you can convert it to a data frame using data2 &lt;- as.data.frame(data)</p>"},{"location":"day1/#looking-at-some-unknown-data","title":"Looking at some unknown data","text":"<p>The data for this exercice is provided in an Excel file, data.xls. You need to export this files from Excel to either CSV or text (tab-delimited) files, and then read it in R using one of the following commands: <pre><code>data &lt;- read.table(\"data.txt\", header=TRUE)  # Reads a tab-delimited file and tells R that\n# the first line actually contains a header\n</code></pre></p> <pre><code>data &lt;- read.csv(\"data.csv\")                 # Reads a CSV file\n</code></pre> <p>The file contains three datasets in three columns of the file. Start by looking at some summaries of the data:</p> <pre><code>data\nsummary(data)\nsd(data[,1]); sd(data[,2]); sd(data[,3])\n</code></pre> <p>What comment can you make about these datasets ?</p> <p>The individual datasets can be accessed by using one of the (equivalent) commands</p> <pre><code>data$data1   # Column named \"data1\"\ndata[,1]     # First column (= column \"data1\")\n</code></pre> <p>It may be easier to copy them in separate variables: <pre><code>data1 &lt;- data$data1\nsummary(data1)\n</code></pre> or, equivalently: <pre><code>attach(data)\nsummary(data1)\n</code></pre></p> <p>While these numbers are interesting, they are only a very short summary of the data, as you know by now. We are going to plot the data in several different ways. Firstly, let us plot the usual barplot with standard deviation; is it very informative ?</p> <pre><code>means &lt;- as.vector(colMeans(data))  # means for the 3 datasets\nsds &lt;- as.vector(sapply( data, sd))      # SDs for the 3 datasets\n# bp will contain the x coordinates of the three barplots\n# ylim is used to make sure that some space is left for the error bar\nbp &lt;- barplot(means, ylim=1.1*range(0, means+sds), names.arg=c(\"Data1\", \"Data2\", \"Data3\"))\narrows(as.vector(bp), means, as.vector(bp), means+sds, angle=90, code=3)\n</code></pre> <p>Let us look at 4 different ways of plotting the data. In the case of the histogram, you can change the number of bars if necessary by adding the argument breaks=n.</p> <pre><code>datatoplot &lt;- data[,1]\n</code></pre>"},{"location":"day1/#plot-4-rows-of-graphs-on-one-plot","title":"Plot 4 rows of graphs on one plot","text":"<pre><code>par(mfrow=c(4,1))\n</code></pre> <p>1st plot: individual points on the x-axis; random noise on the y-axis so that points are not too much superimposed <pre><code>plot(datatoplot, runif( length(datatoplot), -1, 1), xlim=range(datatoplot))\n</code></pre></p> <p>2nd plot: histogram, with the density line superimposed <pre><code>hist(datatoplot, freq=F, xlim=range(datatoplot))\nlines(density(datatoplot))\n</code></pre></p> <p>3rd plot: average +/- Sd <pre><code>plot(mean(datatoplot), 0, xlim=range(datatoplot), main=\"Mean and standard deviation of a\")\narrows(mean(datatoplot)-sd(datatoplot), 0, mean(datatoplot)+sd(datatoplot), 0, angle=90, code=3)\n</code></pre></p> <p>4th plot: boxplot <pre><code>boxplot(datatoplot, horizontal=TRUE, ylim=range(datatoplot))\n</code></pre></p> <p>Do these plots for the three different datasets. Are there cases where some plots are more adapted to the data than others ? What about the number of bars in the histograms ?</p>"},{"location":"day1/#testing-the-mean-of-a-normally-distributed-dataset","title":"Testing the mean of a normally distributed dataset","text":"<p>This is a small exercise to understand normally distributed data and t-tests.</p> <p>First, let us create a set of normally distributed data. In R, one can generate randomly normally distributed data using the rnorm function where one can choose the mean and the standard deviation. Here, we choose it to be 0.</p> <pre><code> x &lt;- rnorm(10000,mean=0, sd=1)\n</code></pre> <p>Visualise the data using histograms. </p> <pre><code>hist(x)\n</code></pre> <p>test for statistical difference and try to understand the output, i.e is the mean different from 0?</p> <p><pre><code>t.test(x)\n</code></pre> Find a way to get the p-value for this test</p> Answer <pre><code>t.test(x)$p.value\n</code></pre> <p>Repeat this test 10 times. For that create a vector with 10 entries.  Then fill these entries with a loop. </p> <p><pre><code>s &lt;- rep(0,10) # this is an empty vector with 10 entries\nfor(i in 1:10){ # this is a loop, called a \"for\" loop, it will repeat \n              # everything in parenthesis 10 times changing the variable\n              # i from 1 to 10 at each iteration\nx &lt;- rnorm(10000,mean=0, sd=1)\ns[i] &lt;- t.test(x)$p.value # does a t.test then takes the p.value obtained and\n                          # puts it into the i-th entry of s\n\n}\n</code></pre> Print the variable that you created to see if the test got significant by chance.</p> <pre><code>s\n</code></pre> <p>Repeat this with 100 or 1000 iterations. Change the value for the mean and test it.</p>"},{"location":"day1/#looking-at-students-data","title":"Looking at students data","text":"<p>Load the file students.csv into R. It contains data collected from students.</p> <p>Look at the variables; try to know/explore the data: summarize the different variables numerically and graphically, and see if you can find relationships between them.</p>"},{"location":"day1/#quitting-r","title":"Quitting R","text":"<p>You will not need to save any R objects that you created today (unless you wish to), so feel free to \u2018clean up\u2019 with rm(). To remove all objects in your workspace (permanently and irreversibly, so be careful), type rm(list=ls()), or simply answer n when asked if you wish to save your workspace image. This question appears on the screen when you quit R; to quit, type</p> <pre><code>q()\n</code></pre> Hint <p>Before quitting, try just typing</p> <p><pre><code>q\n</code></pre> without any parentheses. This might help you to remember that you need the parentheses!   </p>"},{"location":"day2/","title":"Hypothesis Testing","text":""},{"location":"day2/#learning-outcomes-of-the-day","title":"Learning outcomes of the day","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand parametric vs non parametric tests</li> <li>Differences between one-sided/two-sided/paired/one-sample/two-sample t-tests</li> <li>Understand how to analyse multiple group comparisons</li> </ul>"},{"location":"day2/#material","title":"Material","text":"<p>Slides of lectures:</p> <p>Download slides Morning Download slides Afternoon</p> <p>The purpose of this exercise is to help you to better interpret a p-value by using R for introducing you to some simple hypothesis testing functions. As usual, be sure to read the help documentation for any new functions.</p>"},{"location":"day2/#exercise-1-one-sample-t-test","title":"Exercise 1: One-sample t-test","text":"<p>We use the intake data, available in the ISwR package. We will use the variable pre.</p> <pre><code>library(ISwR)\nlibrary(ggplot2) #we will show you how to do some standard ggplots\nlibrary(ggpubr)\nlibrary(rstatix)\ndata(intake)\n?intake\nattach(intake)\nintake\n</code></pre> <p>Start off by looking at some simple summary statistics: mean, sd, quantile (hardly necessary for such a small data set, but good practice).</p> Answer <pre><code>summary(intake)\nmean(intake$pre)\nsd(intake$pre)\n</code></pre> <p>Might these data be approximately normally distributed? </p> Answer <pre><code># Assumption: no significant outliers in the data\nggboxplot(intake$pre, width = 0.5, add = c(\"mean\",\"jitter\"),ylab = \"premenstrual intake\", xlab = F)\n\nidentify_outliers(as.data.frame(intake$pre))\n\n# Assumption: normality\nggqqplot(intake,\"pre\")\nshapiro_test(intake$pre)\n</code></pre> <p>Suppose you wish to test whether there is a systematic deviation between the women\u2019s (pre) energy intake and a recommended value of 7725 kJ. Assuming that the data are from a normal distribution, we are interested in testing whether the (population) mean is 7725. We can do a t-test in R as follows:</p> <p><pre><code>t.test(pre, mu=7725)\n</code></pre> Any idea what the argument alternative is doing ? </p> <pre><code>t.test(pre, mu=7725, alternative=\"less\")\n</code></pre> <p>There are several components to the output, Take some time to make sure you can understand what it all means. For an alpha level of 0.05, do you reject the null hypothesis? What about for an alpha level of 0.01?</p> <p>The default assumes that you want a 2-sided test. Use help to find out how you could get a 1-sided test that would be meaningful for the dataset, and carry this out. For an alpha level of 0.01, do you reject the null hypothesis?</p>"},{"location":"day2/#exercise-2-two-sample-t-test","title":"Exercise 2: Two-sample t-test","text":"<p>We use the energy data to illustrate the use of t.test for testing equality of population means based on two independent samples. Here, we wish to compare mean energy expenditure between lean and obese women.</p> <p><pre><code>data(energy)\n?energy\nattach(energy)\nenergy\n</code></pre> What are the assumptions you need to check for carring out a test ?</p> Answer <pre><code># assumption 1: data in each group are normally distributed.\n\nind.obese &lt;- which(energy$stature == \"obese\")\nind.lean &lt;- which(energy$stature == \"lean\")\n\nshapiro_test(energy$expend[ind.obese]) \nshapiro_test(energy$expend[ind.lean]) \n\n# assumption 2: the variances for the two independent groups are equal.\n\nlevene_test(energy, expend~stature)\n# if the command above does not work, use the levene test from the car package\ncar::leveneTest(expend~stature, energy, center = \"median\")\n</code></pre> <p>The variable stature gives the grouping. The test can be carried out as follows: <pre><code>t.test(expend ~ stature)\n</code></pre></p> <p>Check that you understand the output. For an alpha level of 0.01, do you reject the null hypothesis?</p>"},{"location":"day2/#exercise-3-paired-t-test","title":"Exercise 3: Paired t-test","text":"<p>For this exercise we take again the intake data set. For same individuals measurements were taken premenstrual and postmenstrual. Paired tests are used when there are two measurements (a \u2018pair\u2019) on the same individual. A paired test is essentially a one-sample test of the differences between the measurements.</p> <p>Any assumptions to be tested ?</p> Answer <pre><code># assumption 1: Each of the paired measurements must be obtained from the same subject\n# check your sampling design !\n\n# assumption 2: The measured differences are normally distributed.\n\nintake.diff &lt;- intake$post - intake$pre\nintake.diff.df &lt;- as.data.frame(intake.diff)\n\nshapiro_test(intake.diff)\n</code></pre> <p>We can carry out a paired t-test on the differences between pre and post from the intake data as follows: <pre><code>t.test(pre, post, paired=TRUE)\n</code></pre></p> <p>Again, make sure that you know how to interpret the output. Assuming an alpha level of 0.01, what do you conclude?</p> <p>It was important here to tell t.test that this was a paired test. What happens if you leave out paired=TRUE from the t.test command? Are the assumptions for a two-sample test satisfied in this situation?</p>"},{"location":"day2/#exercise-4-bonus-simulation-of-mice-weight-data-and-p-values","title":"Exercise 4: (Bonus) Simulation of mice weight data and p values","text":"<p>We are going to simulate in a very simple way weight data for WT and KO mice. We will use a two-sample t-test for testing the difference in mean weight between the 2 groups of mice.</p> <pre><code>KO &lt;- runif(10, min=27, max=34)\nWT &lt;- runif(10, min=27, max=34)\nKO &lt;- as.data.frame(KO)\nnames(KO) &lt;- \"weight\"\nKO$genotype &lt;- \"KO\"\nWT &lt;- as.data.frame(WT)\nnames(WT) &lt;- \"weight\"\nWT$genotype &lt;- \"WT\"\n\nKO_WT &lt;- rbind(KO,WT)\n\nboxplot(KO_WT$weight ~ KO_WT$genotype, main=\"Mice weight at 18 weeks\", xlab=\"\", ylab=\"\")\n\nres.welch.test &lt;- t.test(KO_WT$weight ~ KO_WT$genotype)\nres.t.test &lt;- t.test(KO_WT$weight ~ KO_WT$genotype, var.equal = T)\n</code></pre> <p>Display or summarise the resulting p-values.</p> <p>We will now simulate weight data for WT and KO mice 1000 times and look at the distribution of p-values:</p> <pre><code>sim.p.welch.test &lt;- NULL\nsim.p.t.test &lt;- NULL\n\nfor (i in 1:1000) {\n  KO &lt;- runif(10, min=27, max=34)\n  WT &lt;- runif(10, min=27, max=34)\n  KO &lt;- as.data.frame(KO)\n  names(KO)&lt;- \"weight\"\n  KO$genotype &lt;- \"KO\"\n  WT &lt;- as.data.frame(WT)\n  names(WT)&lt;- \"weight\"\n  WT$genotype &lt;- \"WT\"\n\n  KO_WT &lt;- rbind(KO,WT)\n\n  res.welch.test &lt;- t.test(KO_WT$weight ~ KO_WT$genotype)\n  res.t.test &lt;- t.test(KO_WT$weight ~ KO_WT$genotype, var.equal = T)\n\n  sim.p.welch.test &lt;- c(sim.p.welch.test, res.welch.test$p.value)\n  sim.p.t.test &lt;- c(sim.p.t.test, res.t.test$p.value)\n}\n\nsum(sim.p.welch.test &lt; 0.05)\nsum(sim.p.t.test &lt; 0.05)\n</code></pre> <p>How many tests are significant ? What if you apply a Bonferroni correction ? What if you apply a FDR correction ? (use the p.adjust function if needed).</p> <p>Change the parameters of the simulations and see what is the effect on the p-values.</p> Answer <pre><code>adj.bonf &lt;- p.adjust(sim.p.welch.test, method=\"bonf\")\nsum(adj.bonf &lt; 0.05)\nadj.BH &lt;- p.adjust(sim.p.welch.test, method=\"BH\")\nsum(adj.BH &lt; 0.05)\n</code></pre>"},{"location":"day2/#exercise-5-anova","title":"Exercise 5: ANOVA","text":"<p>Install the faraway and the tidyverse packages</p> <pre><code>library(faraway)\nlibrary(tidyverse)\ndata(coagulation)\n</code></pre> <p>The dataset comes from Faraway (2002) and comprises a set of 24 blood coagulation times. 24 animals were randomly assigned to four different diets and the samples were taken in a random order.</p> <ol> <li>Load the data and explore the dataset</li> </ol> Answer <pre><code>data(coagulation)\n\n# check the data\nsummary(coagulation)\n\ncoagulation %&gt;% group_by(diet) %&gt;% get_summary_stats(coag, type = \"mean_sd\") \n\nboxplot(coagulation$coag ~ coagulation$diet)\n\nggboxplot(coagulation, x=\"diet\",y=\"coag\")\n</code></pre> <ol> <li>Fit an ANOVA model, this also means checking assumptions!</li> </ol> Answer <pre><code># check normality\n\n?ggqqplot\nggqqplot(coagulation[coagulation$diet==\"A\",], \"coag\")\nggqqplot(coagulation[coagulation$diet==\"B\",], \"coag\")\nggqqplot(coagulation[coagulation$diet==\"C\",], \"coag\")\nggqqplot(coagulation[coagulation$diet==\"D\",], \"coag\")\n\nshapiro.test(coagulation[coagulation$diet==\"A\",\"coag\"])\nshapiro.test(coagulation[coagulation$diet==\"B\",\"coag\"])\nshapiro.test(coagulation[coagulation$diet==\"C\",\"coag\"])\nshapiro.test(coagulation[coagulation$diet==\"D\",\"coag\"])\n\n# check variance equality\nlevene_test(coagulation,coag~diet)\n# if the command above does not work, use the levene test from the car package\ncar::leveneTest(coag~diet, coagulation, center = \"median\")\n\n# do anova\nanova_diet &lt;- aov(coagulation$coag~coagulation$diet)\n\nsummary(anova_diet)\n</code></pre> <ol> <li>Is there some differences between the groups? If yes, which group(s) is different ?</li> </ol> Answer <pre><code># check pairwise \n\nTukeyHSD(anova_diet)\n\npairwise.t.test(coagulation$coag,coagulation$diet,p.adj=\"bonf\")\npairwise.t.test(coagulation$coag,coagulation$diet,p.adj=\"holm\")\n</code></pre>"},{"location":"day3/","title":"Linear Models","text":""},{"location":"day3/#learning-outcomes-of-the-day","title":"Learning outcomes of the day","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand association between variables</li> <li>Understand correlation and how this is related to the scatterplot</li> <li>Understand simple and multiple regression </li> </ul>"},{"location":"day3/#material","title":"Material","text":"<p>Slides of lectures:</p> <p>Download slides morning Download slides afternoon</p> <p>The purpose of these exercises is to introduce you to using R for modeling. Applications are the estimation of parameter values, the determination of variables that are associated with each other, for example the identification of important biological factors that affect a given outcome.</p>"},{"location":"day3/#exercise-class","title":"Exercise class","text":"<p>Go to the right directory and load the file class.csv</p> <pre><code>class&lt;- read.csv(\"class.csv\")\n</code></pre> <p>inspect the data</p> <pre><code>dim(class)\nhead(class)\nsummary(class[,-1])\n</code></pre> <p>Problem the data set does not have column of Gender as a factor <pre><code>class &lt;- as.data.frame(class)\nclass$Gender &lt;- as.factor(class$Gender)\n</code></pre></p> <p>inspect the pairs of variables on 2by2plots <pre><code>pairs(class[,-c(1)])\n</code></pre></p> <p>Investigating Height ~ Age without the attach function</p> <pre><code>Height2 &lt;- class$Height\nAge2&lt;- class$Age \nlm( Height2 ~ Age2)\n</code></pre> <p>or using the data slot </p> <pre><code>lm( Height ~ Age, data=class) \nmodel &lt;- lm( Height ~ Age, data=class) \nmodel\n</code></pre> <p>plot the age vs Height and add the fitted line with abline <pre><code>plot( class$Age, class$Height)\nabline(model, col=\"red\", lwd=2)\n</code></pre></p> <p>change the range of xaxis and yaxis in order to visually see the intersept <pre><code>plot(class$Age, class$Height, \n      xlim=range(0,class$Age), \n      ylim=range(coef(model)[1], class$Height))\nabline(model, col=\"red\", lwd=2)\n</code></pre></p> <p>Do a summary of the linear model to see the residuals the degrees of freedom and the p-values estimated <pre><code>summary( lm( Height ~ Age, data = class) )\n</code></pre></p>"},{"location":"day3/#anscombe-data","title":"Anscombe data","text":"<p>We will start by exploring the classic Anscombe example data set which comes with R. You will soon see why it is \u2018instructive\u2019. It has been created for the didactic purpose to make people analysing data aware of the fact that while correlation is very convenient and useful, one should know certain limitations. This analysis will demonstrate the importance of examining scatterplots as part of data analysis. First we load the data.</p> <pre><code>data(anscombe)\n</code></pre> <p>This is a small dataset, so we can look at the whole data. For large datasets we would just look at the dimension (dim) of the data frame and at a few lines, here [1:3, ] selects the first three rows (and all their columns). Then we get some summary information for the 8 columns (four X,Y pairs), the summary function automatically executes on each column and returns the mean and the \u201cfive numbers summary\u201d (minimum, maximum, 1st 2nd and 3rd quartile, the 2nd is the median) Similarly, we make the boxplots, for a graphical visualization.</p> <pre><code>anscombe\ndim(anscombe)\nanscombe[1:3, ]\nsummary(anscombe)\nboxplot(anscombe)\n</code></pre> <p>Question Q1: What do you notice about the summary statistics? (Example answer given later)</p> <p>Now, we \u201cattach\u201d the data, we can then use the variable names used in the data (the names of the columns) directly in the R console window. <pre><code>attach(anscombe)\n</code></pre></p> <p>Computing correlations; Correlations and scatterplots This data set consists of four pairs of X,Y data: x1 and y1 go together, etc. Find the correlation coefficient for each of the four sets. You can either do this for each pair separately by hand, e.g.</p> <pre><code>cor(x1,y1)\ncor(x2,y2)\ncor(x3,y3)\ncor(x4,y4)\n</code></pre> <p>etc., or all at once (cor applies to all pairwise combinations of columns), and the result of cor is given to the function round specifying that we want rounding to 3 decimal places.</p> <pre><code>round(cor(anscombe) , digits=3)\n</code></pre> <p>Anything interesting here? Guess what the four scatterplots will look like....</p> <p>Done guessing? :)</p> <p>OK, now for the test - go ahead and make the four scatterplots. The first command subdivides the graphical window into 2x2 panels, each with a square plotting region.</p> <pre><code>par(mfrow=c(2,2),pty=\"s\")\nplot(x1,y1)\nplot(x2,y2)\nplot(x3,y3)\nplot(x4,y4)\n</code></pre> <p>Were you right? Are you surprised??</p>"},{"location":"day3/#linear-regressions","title":"Linear regressions","text":"<p>We can add regression lines (abline plots straight lines) and a title to the plots as follows:</p> <pre><code>plot(x1,y1)\nabline(lm(y1 ~ x1))\ntitle(\"Plot 1\")\n\nplot(x2,y2)\nabline(lm(y2 ~ x2))\ntitle(\"Plot 2\")\n\nplot(x3,y3)\nabline(lm(y3 ~ x3))\ntitle(\"Plot 3\")\n\nplot(x4,y4)\nabline(lm(y4 ~ x4))\ntitle(\"Plot 4\")\n</code></pre> <p>Moral of the story: there is only one way to tell what the scatterplot will look like: you have to look at it! Even the results of the statistical estimation of intercept and slope, based on assumed normal distribution of the residuals, can be misleading.</p> <p>Question Q2: Which of the data correspond to which of the four comments?: A) One single point drives the correlation to a higher value B) One single point drives the correlation to a lower value C) A case where the straight line seems the appropriate model D) The graph departs from a straight line, shows curvature, the straight line seems unsuitable in the sense that a better model exists.</p> <p>What do you expect for the estimation of the slopes? Will the P values and the confidence intervals be comparable? The summary function applied to the object returnd by lm</p> <pre><code>summary(lm(y1 ~ x1))\nsummary(lm(y2 ~ x2))\nsummary(lm(y3 ~ x3))\nsummary(lm(y4 ~ x4))\n</code></pre> <p>Here one could compute hat values to see if they can identify points with a high influence on the estimates. But this is planned already for another example.</p> <p>Question Q3: Can you find a more appropriate model for (x2,y2) ?</p> Don\u2019t open this before having tried to answer <p>Solution to Questions: Q1: With the exception of x4, the x columns have almost identical numerical summaries, and so have the y columns.</p> <p>Q2: Pairs Description-Plot: A-4, B-3, C-1, D-2</p> <p>The first plot seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear, and the Pearson correlation coefficient is not relevant. In the third case, the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.81. Finally, the fourth example shows another example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear.</p> <p>Q3: The graph resembles that of a quadratic function. We can use a polynomial including both x and x^2: <pre><code>z2 &lt;- x2^2\nsummary(lm(y2 ~ x2 + z2 ))\n</code></pre></p> <p>The summary shows that the results of these models fit better to the given points.</p> <p>You cannot use the abline function to plot the resulting curve on the plot, as it is not a straight line anymore. However, you can use the curve function, using the three coefficients returned by the summary:</p> <pre><code>plot(x2, y2)\ncurve( -5.99+2.78*x-0.127*x^2, add=TRUE)\n</code></pre> <p>Alternatively, you can extract the coefficients directly:</p> <pre><code>coefs &lt;- coef( lm(y2 ~ x2 + z2) )\ncurve( coefs[1] + coefs[2]*x + coefs[3]*x^2, add=TRUE)\n</code></pre>"},{"location":"day3/#thuesen-data","title":"Thuesen data","text":"<p>Exploring bivariate data We will use the \u2018Thuesen\u2019 data (L. Thuesen, Diabetologica 1985) contained in the ISwR package. (book \u201cIntroductory Statistics with R\u201d by P. Dalgaard). This data set contains fasting blood glucose concentration (mmol/l, normal range 4-6 mM, except shortly after eating) and mean circumferential shortening velocity of the left ventriculum (percentages of circumference / s, average in the healthy control group was 0.85) measured for 24 (type 1-) diabetic patients. Both variables are numeric, continuous.</p> <p>Diabetes is considered a proven a risk factor for heart disease. The glucose concentration is increased and the degree of increase is an indicator of how well the person is managing to control it (\u2018metabolic control\u2019), for example by controlling sugar uptake. The shortening velocity is an indicator of cardiac function. It is increased in diabetic persons and believed to be a risk factor for heart valve function impairment.</p> <p>A question of interest here is if better metabolic control - as shown by lower (more normal) blood glucose level - is associated with lower (more normal) shortening velocity. The interest was in testing, if there is a relation between these two physiologial quantities. If so, it would suggest that a diabetic person that can keep its glucose low might have less risk of suffering later from heart diseases.</p> <pre><code>library(ISwR)\ndata(thuesen)\n</code></pre> <p>We can start by looking at the data: (You would not want to do this for very large data sets!)</p> <pre><code>thuesen\n</code></pre> <p>There are several ways to access the individual variables in thuesen:</p> <ol> <li>We could use the $ operator: \u2018name of the data - $ - name of the variable\u2019:</li> </ol> <pre><code>thuesen$short.velocity\nthuesen$blood.glucose\n</code></pre> <ol> <li>Or we could use the subset operator [ to select individual columns by name or by position:</li> </ol> <pre><code>thuesen[,\"blood.glucose\"]\nthuesen[,2]\n</code></pre> <ol> <li>Or we could attach the data and just refer to the variables by their names:</li> </ol> <pre><code>attach(thuesen)\nshort.velocity\nblood.glucose\n</code></pre> <ol> <li>This is still a lot of typing! We could assign the variables to new ones with shorter names:</li> </ol> <pre><code>sv &lt;- thuesen$short.velocity\nbg &lt;- thuesen$blood.glucose\n</code></pre> <p>Get some univariate summaries <pre><code>summary(thuesen)\n</code></pre></p> <p>If you are interested in knowing just the means, you can use the function colMeans(thuesen).</p> <p>You might have noticed that one of the observations was \u2018NA\u2019 (i.e. Not Available). In case you did not notice this, it will be signaled to you if you use certain functions on the data (e.g. mean, sd). You must specify that you want the observations with NA values removed:</p> <pre><code>colMeans(thuesen, na.rm=TRUE)\n</code></pre> <p>Look at a bivariate summary If you type:</p> <pre><code>cor(thuesen)\n</code></pre> <p>you will get an error: Error in cor(thuesen) : missing observations in cov/cor. [You will have seen a similar message above when calculating the means.] This is because of the missing value. We can learn which argument to set in cor to decide how to handle the missing values, by using the help facility \u2018?\u2019 to ask for the documentation for this function. Here we use the \u2018use\u2019 argument to exclude those observations that have one or more missing values from the calculation of the correlation.</p> <pre><code>?cor\nround( cor(thuesen, use=\"complete.obs\"), 4)\n</code></pre> <p>Q1) What is the correlation between blood glucose and shortening velocity? What do you think the scatter plot will look like? Well, we just learned that the only way to know is to look!</p> <p>Look at the scatter plot We will be interested in predicting shortening velocity from blood glucose, so our Y variable is sv and X variable is bg:</p> <pre><code>plot(bg, sv)\n</code></pre> <p>We can enhance the plot in a number of ways, for example changing the plotting character (pch), adding different axis labels (xlab, ylab): <pre><code>plot(bg, sv, pch=16, xlab=\"Blood Glucose\", ylab= \"Shortening Velocity\")\n</code></pre></p> <p>Finding the Regression Line Use lm to estimate the regression line from the data:</p> <pre><code>th.lm &lt;- lm(sv ~ bg)\n</code></pre> <p>We can get enough information to write out the regression line from:</p> <pre><code>th.lm\n</code></pre> <p>However, we will see a more comprehensive summary using \u2018summary\u2019 : <pre><code>summary(th.lm)\n</code></pre></p> <p>Q2) Using this information, write out a formula for the regression line. Are any coefficients significant at the 5% level? (could you estimate a 95% confidence interval?) Is sv predicted to be lower when bg is lower, or not?</p> <p>We can add the regression line to the scatter plot: <pre><code>abline(lm(sv ~ bg))\n</code></pre></p> <p>Exploring Model Fit To explore the fit of the model, we should examine the residuals more carefully. First, extract the residuals from th.lm using the \u2018extractor\u2019 resid function:</p> <pre><code>th.resid &lt;- resid(th.lm)\n</code></pre> <p>Normality First, extract the residuals from th.lm. We eliminate the observations with missing values for sv (using is.na(sv)) and the \u2018not\u2019 operator \u2018!\u2019. Then we produce QQ plots for the residuals.</p> <pre><code>bg.1 &lt;- bg[! is.na(sv)]\nplot(bg.1,th.resid)\nabline(h=0)\n\nqqnorm(th.resid)\nqqline(th.resid)\n</code></pre> <p>Q3) Are any points off the line to indicate departure from a normal distribution?</p> <p>Constant variance We check this assumption by plotting the residuals against the fitted values that we obtain by applying the function of this name to the lm object. <pre><code>th.fv &lt;- fitted.values(th.lm)\nplot(th.fv, th.resid)\nabline(h=0)\n</code></pre></p> <p>Q4) Does the variance of the residuals seem roughly constant across the range of x? If not, what pattern do you see?</p> <p>Influential points Here we compute the \u2018hat\u2019 values (function lm.influence, variable hat in the object returned) We want to compare the hat values to 2p/n or 3p/n, where p is the dimension of the model space (2 here). Values bigger than this may be considered \u2018influential\u2019. Note the use of \u2018#\u2019, this symbol and the rest of the line are ignored by R. It is useful to add small comments to our R code (and to keep this together in a protocol file).</p> <pre><code>th.hat &lt;- lm.influence(th.lm)\nsort(th.hat$hat) # look at the sorted values\nindex &lt;- seq(1:length(th.hat$hat)) # integers 1 .. number of points\nplot(index,th.hat$hat,xlab=\"Index\",ylab=\"Hat value\", ylim=c(0,0.3)) # ylim sets the range of the y-xis\nabline(h=c(2*2/23,3*2/23),lty=c(2,3),col=c(\"blue\",\"red\") ) # h for horizontal lines, here two specified together\n</code></pre> <p>Q5) Do there appear to be any influential points beyond the two horizontal lines?</p> <p>Try to find the points with the highest leverage</p> Answer <pre><code>th.highlev &lt;- c(4,13) # should be 4 , 13\n</code></pre> <p>We can get the glucose and velocity measurements for these points by typing <pre><code>thuesen[th.highlev,]\n</code></pre></p> <p>Let\u2019s see where these points are on the original scatter plot. We\u2019ll plot these points as large (scale \u2018cex=2\u2019) blue dots:</p> <pre><code>plot(bg, sv, pch=16, xlab=\"Blood Glucose\", ylab= \"Shortening Velocity\")\nabline(lm(sv ~ bg))\npoints(bg[th.highlev],sv[th.highlev],pch=16,col=\"blue\",cex=2)\n</code></pre> <p>Why do you think these points are the most influential?</p> Don\u2019t open before having tried to answer <p>Q1) cor r = 0.4168</p> <p>Q2) The results indicate high significance for a test of the hypothesis \u2018intercept = 0\u2019 and are just barely significant for a test on the \u2018coefficient = 0\u2019 for bg. An association between bg and sv can be assumed given these data, but the evidence is not overwhelming strong and caution in interpretation is required, especially if there might be doubts as to the correctness of the model assumption being made. regression line: sv = 1.09781 + 0.02196 * bg predicts on average an increase of sv with bg Estimated 95% CI for the coefficients of bg: Half-width: 1.96 * 0.01045 = 0.0205 (using the 1.96 from the normal distribution) CI: 0.0222 +/- 0.0205 = 0.0017, 0.0427 Half-width: 2.08 * 0.01045 = 0.0217 (using the t distribution with 21 degrees of freedom) CI: 0.0222 +/- 0.0217 = 0.0005, 0.0439 The slope coefficient is significant in the hypothesis test agains slope=0, but the confidence intervals include values very close to 0. While it is predicted that lower bg is associated with lover sv, how strong this effect is remains relatively unclear.</p> <p>Q3) The five points on the right hand side are fairly far from the straight line and suggest a departure from normal distribution that is important enough to be cautious about drawing definitive conclusions.</p> <p>Q4) The variance seems roughly constant, but a bit higher on the right hand side for the larger fitted values. There can be a bit of concern about model validity.</p> <p>Q5) There are two points with a high influence (leverage) They are obove the 2p/n but below the 3p/n line. Again, this is a reason of concern. Probably because they have the highest Blood glucose and not enough points in the range 15-20.</p> <p>In summary, the data present evidence for a linear association of higher ventricular contraction velocity when blood glucose is higher, but with substantial unexplained variation in the model (explained variance by R squared below 0.2, test on coefficient for bg borderline to significance). Because the data suggest a relation, but no final conclusion can be strongly supported, the recommendation is to sample more observations. In particular the relation is not well estimated at higher values of bg, and more observations with bg &gt; 13 would be useful.</p>"},{"location":"day3/#expression-data","title":"Expression data","text":"<p>We are going to use data from a breast cancer study, described in the paper by C. Sotiriou et al, \u201cGene expression profiling in breast cancer: understanding the molecular basis of histological grade to improve prognosis\u201d, J Natl Cancer Inst (2006 Feb 15;98(4):262-72). File expression-esr1.csv contains microarray data for 20 different patients and 9 different probes. The 9 probes interrogate the same gene, ESR1 (Estrogen Receptor). The expression data is log transformed. File clindata.csv contains clinical data about these 20 patients. In particular, the \u201cer\u201d column indicates whether the tumour was identified (using immunohistochemistry) as expression an estrogen receptor. In theory, this information at the protein level should match the information obtained from RNA: an er value of 1 should correspond to a high expression value for gene ESR1, while a value of 0 should correspond to a low value. Answer the following questions (using statistical and graphical arguments).</p> <p>Do the 9 probes give similar results, or can you identify one or several clusters of probes that give similar results ? Do the probes give similar results than those obtained by immunohistochemistry ? What is the probe that is most closely associated with the immunihistochemistry data ? You can use the cor() command on a table of variables and which will calculate all pairwise correlations; the pairs() command will create the related graphics.</p> Answer <pre><code>##open it and look at it \nclin &lt;- read.csv(\"clindata.csv\")\nexpr &lt;- read.csv(\"expression-esr1.csv\")\nhead(clin)\nsummary(clin)\nclin$er &lt;- as.factor(clin$er)\nclin$treatment  &lt;- as.factor(clin$treatment )\nrownames(clin)&lt;-clin[,1]\nsummary(clin)\nhead(expr)\ndim(expr)\n    ##check all the correlation (Pearson) and see which one correspond\ncor(expr)\n\n##for loop to see all the correlations with ER status\nfor(i in 1:9){\nprint(cor(expr[,i],as.numeric(clin[rownames(expr),\"er\"]),method=\"spearman\"))\n    }\n    ### put them in a variable\n    s &lt;- c()\n    for(i in 1:9){\n    s[i] &lt;-cor(expr[,i],as.numeric(clin[rownames(expr),\"er\"]),method=\"spearman\")\n    }\n    head(clin)\n\n    ### do some boxplots\nplot(clin$er,expr[,1])\n</code></pre>"},{"location":"day3/#bonus-exercice","title":"Bonus Exercice","text":"<p>Already done ? You can have a look at the advanced topics with more exercises and interesting subjects here. </p> <p>Or try to understand the multiple regression with categorical variables and interaction between them.</p> <p>For that have a look at the chapter Interactions in the book from the genomics class\u2019 github  http://genomicsclass.github.io/book/</p> <p>The data comes from this paper.</p>"},{"location":"day4/","title":"Clustering and Principal component analysis","text":""},{"location":"day4/#learning-outcomes-of-the-day","title":"Learning outcomes of the day","text":"<p>After having completed this chapter you will be able to:</p> <ul> <li>Understand major clustering methods</li> <li>Understand hierarchical clustering</li> <li>Understand principal component analysis</li> </ul> <p>Slides of lectures:</p> <p>Download slides</p>"},{"location":"day4/#the-iris-data-set","title":"The Iris data set","text":"<p>We will study the iris data set, containing measurements of the petal length and width and the sepal length and width for 150 iris flowers of three different types. The data set is available within R. First, we load the data, display its structure and summarize its content. </p> <pre><code>data(iris) \nstr(iris) \nsummary(iris) \nhead(iris) \n</code></pre> <p>Are the range of values for the individual variables comparable? </p> <p>What is the dimensionality of the data (the number of coordinates used to represent the samples)? </p> <p>Next, we create univariate plots to visualize the variables individually or pairwise.  <pre><code>boxplot(iris[, 1:4]) \npairs(iris[,1:4], col = iris$Species, pch = 19) \n</code></pre></p> <p>What do the colors represent? Are there any apparent associations between the variables? </p> <p>We can also create boxplots of the individual variables stratified by iris type.  <pre><code>boxplot(iris$Petal.Width ~ iris$Species) \n</code></pre></p> <p>We can also use hierarchical clustering and the library ComplexHeatmap to generate complex and nice plots. Look up the help, there you can browse through all possible ways to increase the look of the heatmap. </p> <pre><code>library(ComplexHeatmap)\nlibrary(circlize)\ncol_fun &lt;- colorRamp2(c(-2, 0, 2), c(\"blue\", \"white\", \"red\"))\n\nspecies_annot &lt;- HeatmapAnnotation(\n  Species = iris$Species,\n  col = list(Species = c(\n    setosa = \"gold\",\n    versicolor = \"lightgreen\",\n    virginica = \"lightblue\"\n  )),\n  annotation_name_side = \"left\"\n)\n\nHeatmap(\n  t(scale(iris[, -5])),\n  name = \"Scaled value\",\n  top_annotation = species_annot,\n  col = col_fun\n)\n</code></pre> <p>Find which parameters define the method for the hierarchical clustering and change the default behavior.</p> Answer <pre><code>Heatmap(\nt(scale(iris[, -5])),\nname = \"Scaled value\",\ntop_annotation = species_annot,\ncol = col_fun,\nclustering_method_columns =\"average\",\nclustering_distance_columns = \"pearson\")\n</code></pre> <p>Now, we apply PCA to the data, using the prcomp function. Make sure that you know what the arguments to the function stand for. </p> <pre><code>cols &lt;- c(setosa = \"gold\", versicolor = \"lightgreen\", virginica = \"lightblue\")\npca.iris.cov = prcomp(iris[, 1:4], center = TRUE, scale. = FALSE) \nplot(pca.iris.cov$x, col = cols[iris$Species], pch = 19) \n</code></pre> <p>What can you see in the sample plot? </p> <p>In addition to the sample configuration, we also look at the variable loadings, to see which variables contribute most to the principal components, and at the fraction of variance that is explained by each principal component. </p> <pre><code>pca.iris.cov$rotation \nsummary(pca.iris.cov) \n</code></pre> <p>Which variables have the strongest influence on each of the first two principal components? How can you use this information to interpret the sample PCA representation? </p> <p>We can now visualize the sample scores and the variable loadings together in a biplot.  <pre><code>biplot(pca.iris.cov, scale = 0) \n</code></pre></p> <p>What do the different objects in the biplot represent? How are they connected to the output from prcomp? </p> <p>The sample scores from PCA are obtained as linear combinations of the four measured variables, with weights given by the variable loadings. Verify that this is the case e.g. for the scores for the first flower. </p> <pre><code>iris.centered = scale(iris[, 1:4], center = TRUE, scale = FALSE) # compute linear combination \nscores.sample1 = iris.centered[1, ] %*% pca.iris.cov$rotation # compare to PCA scores \npca.iris.cov$x[1, ]/scores.sample1 \n</code></pre> <p>The PCA just applied was based on the non-standardized variables, that is, on the covariance matrix. </p> <p>Where was this specified? Can we gain some understanding of the results (the variable loadings) by studying the individual variable variances? </p> <pre><code>var(iris[, 1:4]) \n</code></pre> <p>Next, we rerun the analysis with standardized variables.  <pre><code>pca.iris.corr = prcomp(iris[, 1:4], center = TRUE, scale. = TRUE) \nplot(pca.iris.corr$x, col = iris$Species, pch = 19) \npca.iris.corr$rotation \nsummary(pca.iris.corr) \nbiplot(pca.iris.corr, scale = 0) \n</code></pre></p> <p>How did the results change? Can we understand the change by using the information form the pairwise scatterplots and the individual variances? Which variables appear to be most important for separating the two groups of iris flowers seen in the PCA plot? </p> <p>To see the amount of variance captured by each of the principal components, construct scree plots for the two PCAs. </p> <pre><code>par(mfrow = c(1, 2)) \nscreeplot(pca.iris.cov, type = \"line\") \nscreeplot(pca.iris.corr, type = \"line\") \n</code></pre> <p>How many principal components are needed to explain most of the variance in the data? </p>"},{"location":"day4/#dengue-fever-example","title":"Dengue fever example","text":"<p>In this example, we will use a gene expression data set aimed at studying the differences between patients with dengue fever and healthy controls or convalescents. The data set corresponds to a publication by Kwissa et al (Cell Host &amp; Microbe 16:115-127 (2014)), and the expression matrix has been deposited in Gene Expression Omnibus (GEO) with accession number GDS5093. </p>"},{"location":"day4/#data-retrieval","title":"Data retrieval","text":"<p>We will use the GEOquery package to retrieve the data and platform annotation information. </p> <p>The GEOquery package is available from Bioconductor, and can be installed in the following way:  <pre><code>if (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"GEOquery\") \n</code></pre> And the data retrieved like this and then have a look at the downloaded data : <pre><code>library(GEOquery)  \n## Retrieve the data from GEO \ngds &lt;- getGEO(\"GDS5093\") \n## If you have already downloaded the data, you can load the soft file directly \n## gds &lt;- getGEO(\"GDS5093.soft.gz\")  \n\n## Look at the elements of the downloaded data head(Columns(gds)) \nhead(Table(gds)) \nhead(Meta(gds))  \n\n## Convert the gds object to an expression set \neset &lt;- GDS2eSet(gds) \n</code></pre></p> <p>We have converted the downloaded object to a so called expression set. The expression set is a special Bioconductor data container that is commonly used to store all information associated with a microarray experiment. Using different accessor functions, we can extract the different types of information stored in the expression set (expression values, sample annotations, variable annotations) </p> <pre><code>## Sample information \nhead(pData(eset))  \n## Expression matrix \nhead(exprs(eset))  \n## Feature information \nhead(fData(eset)) \n</code></pre>"},{"location":"day4/#principal-component-analysis","title":"Principal component analysis","text":"<p>First we perform a PCA using all the variables in the data set. We will perform the following analyses using data on probeset level, but it could also be done after summarizing the expression for all probe sets targeting the same gene.  <pre><code>pca &lt;- prcomp(t(exprs(eset)), scale. = TRUE) \n</code></pre></p> <p>Plot the sample representation from the PCA. How much variance is encoded by each principal component? Are the principal components encoding known information? </p> <pre><code>plot(pca$x, pch = 19, cex = 2) \nplot(pca) \npca$sdev^2/sum(pca$sdev^2)  \nplot(pca$x, pch = 19, cex = 2, col =factor(pData(eset)$disease.state)) \nlegend(\"topright\", legend = levels(factor(pData(eset)$disease.state)),col = 1:4, pch = 19) \n</code></pre> <p>It is common practice to filter out lowly varying genes before standardizing the data set and performing PCA. We will try this approach, using two different filter thresholds, leaving 5,000 and 100 variables, respectively.  <pre><code>vars &lt;- apply(exprs(eset), 1, var) \nvars.order &lt;- order(vars, decreasing = TRUE)  \npca.5000 &lt;- prcomp(t(exprs(eset)[vars.order[1:5000], ]), scale. = TRUE) \nplot(pca.5000$x, pch = 19, cex = 2, col = factor(pData(eset)$disease.state)) \nlegend(\"topright\", legend = levels(factor(pData(eset)$disease.state)), col = 1:4, pch = 19)  \npca.100 &lt;- prcomp(t(exprs(eset)[vars.order[1:100], ]), scale. = TRUE) \nplot(pca.100$x, pch = 19, cex = 2, col = factor(pData(eset)$disease.state)) \nlegend(\"topright\", legend = levels(factor(pData(eset)$disease.state)), col = 1:4, pch = 19) \n</code></pre></p> <p>What do you notice in the last plot? The PC2 clearly splits the samples into two groups. Does this correspond to any known sample annotation? </p> <pre><code>plot(pca.100$x, pch = 19, cex = 2, col = factor(pData(eset)$infection)) \n</code></pre> <p>How can we find out what this is? One way is to look at the genes that are responsible for the split, that is, the ones that are contributing a lot to PC2. </p> <pre><code>pc2.weights &lt;- data.frame(pca.100$rotation[, 2, drop = FALSE]) \npc2.weights$ChromosomeLoc &lt;- fData(eset)[rownames(pc2.weights), \"Chromosome location\"] \nhead(pc2.weights[order(pc2.weights$PC2), ])\ntail(pc2.weights[order(pc2.weights$PC2), ]) \n</code></pre> <p>Now what do you think this component represents? </p>"},{"location":"day4/#golub-leukemia-data","title":"Golub leukemia data","text":"<p>In this exercise we study a microarray data set that is available as an expression set in R, from the library golubEsets. </p> <p>We load the data and extract the expression values and a sample annotation.  <pre><code>library(golubEsets) \ndata(Golub_Train) \ngolub.expr = exprs(Golub_Train) \ngolub.sample.annot = Golub_Train$ALL.AML \n</code></pre></p> <p>First, we apply PCA to the unstandardized data. </p> <pre><code>pca.golub.cov = prcomp(t(golub.expr), center = TRUE, scale. = FALSE) \nplot(pca.golub.cov$x[, 1:2], col = golub.sample.annot, pch = 19) \n</code></pre> <p>We also apply PCA to the standardized data. </p> <pre><code>pca.golub.corr = prcomp(t(golub.expr), center = TRUE, scale. = TRUE) \nplot(pca.golub.corr$x[, 1:2], col = golub.sample.annot, pch = 19) \n</code></pre> <p>Try also to plot the second and third principal components instead of the first two. What may be the reason for the discrepancy between the two results? </p> <p>For high-dimensional data sets like microarrays it is common to filter out the variables with low variance before applying the PCA. Extract the 1,000 variables with the highest variance from the Golub data set, and apply PCA to the standardized data sets to focus mainly on the correlations between the remaining variables. </p> <pre><code>golub.expr.filtered = golub.expr[order(apply(golub.expr, 1, var), decreasing = TRUE)[1:1000], ]\npca.golub.filtered.corr = prcomp(t(golub.expr.filtered), center = TRUE, scale. = TRUE)\nplot(pca.golub.filtered.corr$x[, 1:2], col = golub.sample.annot, pch = 19)\n</code></pre> <p>Compare the resulting plot to the previous two plots. What happens if you apply PCA to the unstandardized, filtered data set? </p>"},{"location":"day4/#swiss-banknote-data","title":"Swiss banknote data","text":"<p>In this exercise we consider a data set containing measurements from 100 genuine Swiss banknotes and 100 counterfeit notes. For each banknote there are six measurements; the length of the bill, the width of the left and right edges, the widths of the bottom and top margin, and the length of the image diagonal. The data set is available in the R package alr3, that is now removed from CRAN, an archived version is still available. Check the precourse, on how to install that library. We also included the banknote as a txt file in the exercises folder </p> <p>First, we load the data and look at the summary  <pre><code>library(alr3)\ndata(banknote) \nsummary(banknote)\n</code></pre></p> <p>Then we apply PCA after scaling each variable to unit variance. </p> <pre><code>pca.banknote = prcomp(banknote[, 1:6], center = TRUE, scale. = TRUE) \nsummary(pca.banknote) \n</code></pre> <p>In this example, we will create a biplot manually, by overlaying the sample and variable PCA plots </p> <pre><code>plot(pca.banknote$x[, 1:2], col = c(\"black\", \"green\")[banknote[, 7] + 1], pch = 19)\narrows(0, 0, 2*pca.banknote$rotation[, 1], 2*pca.banknote$rotation[, 2], col = \"red\", angle = 20, length = 0.1)\ntext(2.4*pca.banknote$rotation[, 1:2], colnames(banknote[, 1:6]), col = \"red\") \n</code></pre> <p>In the resulting plot, the black and green points represent the genuine and counterfeit notes, respectively. The red arrows correspond to the contributions of the six variables to the principal components. </p> <p>In what ways did the counterfeiters fail to mimic the genuine notes? Look at the individual variables to verify. </p> <pre><code>par(mfrow = c(2, 3)) \nfor (v in c(\"Length\", \"Left\", \"Right\", \"Bottom\", \"Top\", \"Diagonal\")) {   boxplot(banknote[, v] ~ banknote[, \"Y\"], xlab = \"Banknote status\", ylab = v) } \n</code></pre>"},{"location":"day4/#cholera-dataset","title":"Cholera Dataset","text":"<p>In 1852, William Farr, published a report of the Registrar-General on mortality due to cholera in England in the years 1848-1849, during which there was a large epidemic throughout the country. Farr initially believed that cholera arose from bad air (\u201cmiasma\u201d) associated with low elevation above the River Thames. John Snow (1855) later showed that the disease was principally spread by contaminated water.</p> <p>This data set comes from a paper by Brigham et al. (2003) that analyses some tables from Farr\u2019s report to examine the prevalence of death from cholera in the districts of London in relation to the available predictors from Farr\u2019s table.</p> <p>First we ask you to load the dataset, which is part of the HistData package in R and load the cholera data set.</p> <pre><code>library(HistData)\nlibrary(ComplexHeatmap)\nlibrary(circlize)\nlibrary(dplyr)\n\ndata(\"Cholera\")\n</code></pre> <p>We then create a variable by selecting only the numerical values from the table (for instance dropping Region information) and scale this data, such that all variable have the same impact. </p> <pre><code>cholera_num &lt;- Cholera[, sapply(Cholera, is.numeric)]\ncholera_num &lt;- na.omit(cholera_num)\n\n# Scale numeric variables (important for k-means)\ncholera_scaled &lt;- scale(cholera_num)\n</code></pre> <p>We will now use one of the popular clustering algorithm called kmeans, which is part of the partitioning clustering (and therefore gives us a cluster number to which each patient belongs to).</p> <pre><code># Run k-means with k = 3 clusters\nset.seed(123)\nkm &lt;- kmeans(cholera_scaled, centers = 3, nstart = 25)\nCholera$cluster &lt;- factor(km$cluster)\n</code></pre> <p>Make different trials how much does the clustering change if you change the nstarts or the number of iterations. What do you observe</p> Answer <pre><code>km &lt;- kmeans(cholera_scaled, centers = 3, nstart = 3)\nCholera$cluster3 &lt;- factor(km$cluster)\nkm &lt;- kmeans(cholera_scaled, centers = 3, iter.max = 10)\nCholera$clustermax10 &lt;- factor(km$cluster)\nkm &lt;- kmeans(cholera_scaled, centers = 3, iter.max=100)\nCholera$clustermax100 &lt;- factor(km$cluster)\n\ntable(Cholera$cluster,Cholera$cluster3)\ntable(Cholera$cluster,Cholera$clustermax10)\ntable(Cholera$cluster,Cholera$clustermax100)\ntable(Cholera$cluster100,Cholera$clustermax10)\n</code></pre> <p>Using the aggregate function in R try to understand the composition of the clusters </p> <pre><code>aggregate(cholera_num, list(cluster = Cholera$cluster), mean)\n</code></pre> <p>Clearly cluster 3 (or maybe another one if you did not set the seed) has a higher cholera_drate than the others. One can then try to understand what it is linked to, by looking at a table with the \u201cwater\u201d information</p> <pre><code>table(Cholera$cluster,Cholera$water)\n</code></pre> <p>We will now do a ComplexHeatmap to illustrate this clustering</p> <pre><code>row_ha &lt;- rowAnnotation(\nCluster = Cholera$cluster,\nWater = Cholera$water,\ncol=   list(Cluster=c(\"1\"=\"blue\",\"2\"=\"red\",\"3\"=\"orange\"), Water=c(\"Battersea\"=\"#AEC6CF\",\"New River\"= \"#FFB7B2\", \"Kew\"= \n  \"#B5EAD7\")),  \nannotation_name_side = \"top\"\n    )\n\n\n row_d &lt;- dist(cholera_scaled)\n row_clust &lt;- hclust(row_d, method = \"ward.D2\")\n\n  Heatmap(\n        cholera_scaled,\n        name = \"z-score\",\n        cluster_rows = row_clust,\n        cluster_columns = TRUE,\n        show_row_names = TRUE,\n        left_annotation = row_ha\n   )\n</code></pre> <p>In conlusion, there seem to be a link between Cholera drate and the water supply of the people. </p>"},{"location":"day4/#points-in-plates-bonus","title":"Points in plates (BONUS)","text":"<ol> <li>Import the data from dataClustering.csv</li> <li>What is the dimension of this dataset?</li> <li>How many data point do we have?</li> </ol> Answer <pre><code>library(\"cluster\") \nmydata1&lt;-read.csv(\"dataClustering.csv\") \ndf&lt;-data.frame(mydata1$Coord_X ,mydata1$Coord_Y ) \ncolnames(df) &lt;- c(\"X\", \"Y\")\nplot(df$X, df$Y, pch=20) \ndim(df)\n</code></pre> <ol> <li>Evaluate Euclidean distance of points in a plates</li> </ol> Answer <pre><code>df.dist&lt;-dist(df) \n</code></pre> <ol> <li>Classify points to find clusters using hierarchical clustering and the average agglomeration method</li> </ol> Answer <pre><code>df.h&lt;-hclust(df.dist,\"ave\") \nplot(df.h) \n\ncolorScale &lt;- colorRampPalette(c(\"blue\", \"green\",\"yellow\",\"red\",\"darkred\"))(1000)\nheatmap(as.matrix(df.dist),Colv=NA, Rowv=NA, scale=\"none\", col=colorScale)\n</code></pre> <ol> <li>We expect to have 3 clusters. When you apply k-means algorithm using 1 iteration, does it differ from applying it using 10 or 100 iterations?</li> </ol> Answer <pre><code>kmeans(df,3)\n\ncl.1 &lt;- kmeans(df, 3, iter.max = 1)\nplot(df, col = cl.1$cluster)\npoints(cl.1$centers, col = 1:5, pch = 8)\n\nkmeans(df,3)\n\ncl.1 &lt;- kmeans(df, 3, iter.max = 1)\nplot(df, col = cl.1$cluster)\npoints(cl.1$centers, col = 1:5, pch = 8)\n\ncl.10 &lt;- kmeans(df, 3, iter.max = 10)\nplot(df, col = cl.10$cluster)\npoints(cl.10$centers, col = 1:5, pch = 8)\n\ncl.100 &lt;- kmeans(df, 3, iter.max = 100)\nplot(df, col = cl.100$cluster)\npoints(cl.100$centers, col = 1:5, pch = 8)\n</code></pre> <ol> <li>What is the outcome of the C-means clustering?</li> </ol> <pre><code>install.packages(\"e1071\")\nlibrary(e1071)\n?cmeans\n</code></pre> Answer <pre><code>cmeans(df,3)\n\ncl.1 &lt;- cmeans(df, 3, iter.max = 1)\nplot(df, col = cl.1$cluster)\npoints(cl.1$centers, col = 1:5, pch = 8)\n\ncl.10 &lt;- cmeans(df, 3, iter.max = 10)\nplot(df, col = cl.10$cluster)\npoints(cl.10$centers, col = 1:5, pch = 8)\n\ncl.100 &lt;- cmeans(df, 3, iter.max = 100)\nplot(df, col = cl.100$cluster)\npoints(cl.100$centers, col = 1:5, pch = 8)\n</code></pre> <ol> <li>What are the top 3 models mclustBIC function suggests based on the BIC criterion?</li> <li>How many clusters did it find using the top model?</li> <li>Plot the outcome</li> </ol> Answer <pre><code>library(\"mclust\")\nBIC &lt;- mclustBIC(df)\nplot(BIC)\nsummary(BIC)\nmod1 &lt;- Mclust(df, x = BIC)\nsummary(mod1, parameters = TRUE)\nplot(mod1, what = \"classification\")\n\nmod2 &lt;- Mclust(df, modelName = c(\"VEE\"))\nplot(mod2, what = \"classification\")\n\nmod3 &lt;- Mclust(df, modelName = \"EEE\",G=9)\nplot(mod3, what = \"classification\")\n</code></pre>"},{"location":"exam/","title":"Exam","text":""},{"location":"exam/#exam","title":"EXAM","text":"<p>The participants who need credits must answer the following questions and send the results as an R script with comments to rachel.marcone@sib.swiss until latest end of 06 February 2026.</p> <p>Data: A set of data collected by Heinz et al.(* Heinz G, Peterson LJ, Johnson RW, Kerk CJ Journal of Statistics Education Volume 11, Number 2 (2003) jse.amstat.org/v11n2/datasets.heinz.html, by Grete Heinz, Louis J. Peterson, Roger W. Johnson, and Carter J. Kerk, all rights reserved) is available in the file IS_24_exam.csv</p> <p>Goals: Get to know the overall structure of the data. Summarize variables numerically and graphically. Model relationships between variables.</p> <p>Download exercise material</p>"},{"location":"exam/#observations","title":"Observations","text":"<ol> <li>Have look at the file in a text editor to get familiar with it</li> <li>Open a new script file in R studio, comment it and save it.</li> <li>Read the file, assign it to object \u201cIS_25_exam\u201d. Examine \u201cIS_25_exam\u201d. a) How many observations and variables does the dataset have ? b) What are the names and types of the variables ? c) Get the summary statistics of \u201cIS_25_exam\u201d.</li> <li>Make a scatter plot of all pairs of variables in the dataset.</li> <li>Calculate the BMI of each person and add it as an extra variable \u201cbmi\u201d to your dataframe (Google the BMI formula).</li> </ol>"},{"location":"exam/#modelling","title":"Modelling","text":"<ol> <li>Is there a significant difference in bmi means between males and females?</li> <li>How strong is the linear (Pearson) correlation between chest girth and height? Is it significant?</li> <li>If you model a linear relationship, how much does the chest girth increase per added cm of height? Is the change significant? What if you do this for males and females separately?</li> <li>Come up with a question for hypothesis testing of your own that includes one or more variable(s) of your choosing from the data set.</li> <li>Make plots as seen in the course to try to give visualization based answers to this question.</li> <li>Test your hypothesis using the tests and modeling techniques from the course, based on the type of variables you have. Include tests of the assumptions where appropriate.</li> </ol>"},{"location":"links/","title":"Links that are useful","text":"<p>https://www.statlearning.com/, a book to be downloaded to learn statistics and apply in R.</p> <p>https://intro2r.com/whoarewe.html, an online book that nicely flows you through R, include also how to do a ggplot</p> <p>the genomics book containing a nice chapter on interaction : http://genomicsclass.github.io/book/, where data comes from that paper : https://www.jstor.org/stable/48576004.</p>"},{"location":"precourse/","title":"Precourse preparation","text":""},{"location":"precourse/#previous-knowledge","title":"Previous knowledge","text":""},{"location":"precourse/#knowledge","title":"Knowledge:","text":"<p>No prior statistical knowledge is required in order to attend the course Participants do not need any experience in R before the course</p>"},{"location":"precourse/#technical","title":"Technical","text":"<p>To do the exercises, you are required to have your own computer with at least 4 Gb of RAM and with an internet connection, as well as the latest the version of R and the free version of RStudio installed.</p>"},{"location":"precourse/#installation-in-r","title":"Installation in R","text":"<p>After installing R and Rstudio, open the Rstudio (by double clicking on the logo). Then, to ensure that the course runs smoothly check if you are allowed to install packages into R. Some companies block users to download packages for security reasons and an error saying : contact your administrator might appear. Make sure to discuss with the IT service of your company if you can, and if not contact us in order to be able to follow the course easily.</p> <p>It might also happen that your antivirus blocks R from downloading packages.</p> <p>In order to check if all runs smoothly, try to download your first package from R Studio, you can go to the menu Tools -&gt; Install packages?, and then choose the package you need installed (choose for example \u201cISwR\u201d). Using the RGui under Windows, you can go to menu Packages -&gt; Install package(s). In the console, you can use the install.packages command: install.packages(\u201cISwR\u201d) for example. Once installed you can load the library, which should then not give you an error: library(\u201cISwR\u201d).</p> <p>If there is any problem, contact us.</p> <p>Below, the code to download all the packages needed for the course.</p> <pre><code># Install BiocManager if not already installed\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\n# CRAN packages\ncran_packages &lt;- c(\n  \"ISwR\",\n  \"ggplot2\",\n  \"ggpubr\",\n  \"rstatix\",\n  \"faraway\",\n  \"tidyverse\",\n  \"circlize\"\n)\n\ninstall.packages(cran_packages)\n\n#as this package was now removed from CRAN\ninstall.packages(\n  \"https://cran.r-project.org/src/contrib/Archive/alr3/alr3_2.0.8.tar.gz\",\n  repos = NULL, type = \"source\"\n)\n\n# Bioconductor packages\nbioc_packages &lt;- c(\n  \"ComplexHeatmap\",\n  \"GEOquery\",\n  \"golubEsets\"\n)\n\nBiocManager::install(bioc_packages)\n</code></pre> <p>Then check if all of them were installed properly</p> <pre><code># Load CRAN packages\nlibrary(ISwR)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(faraway)\nlibrary(tidyverse)\nlibrary(circlize)\nlibrary(alr3)\n\n\n\n# Load Bioconductor packages\nlibrary(ComplexHeatmap)\nlibrary(GEOquery)\nlibrary(golubEsets)\n</code></pre>"}]}